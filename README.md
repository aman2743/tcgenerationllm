# ğŸ¤– Ollama Test Generator

A local LLM-powered test case generator using Ollama (Llama 3.2) with a premium glassmorphic web interface. Generate comprehensive test cases from your code or requirements in real-time.

![Architecture Diagram](./docs/architecture_diagram.png)

## âœ¨ Features

- ğŸ¤– **Local LLM**: Uses Ollama (Llama 3.2) for intelligent test case generation
- ğŸ¨ **Premium UI**: Beautiful glassmorphism design with dark mode
- âš¡ **Real-time Streaming**: Watch test cases being generated live
- ğŸ“‹ **Structured Output**: Professional format with Test ID, Description, Steps, and Expected Results
- ğŸ”’ **Privacy First**: All processing happens locally - no data sent to external APIs
- ğŸš€ **Fast & Efficient**: Leverages local GPU acceleration via Ollama

## ğŸ—ï¸ Architecture

This project follows the **B.L.A.S.T.** protocol for reliable, deterministic automation:

### System Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HTTP POST        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      Streaming      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚   /api/generate       â”‚                  â”‚      Request        â”‚                 â”‚
â”‚  User Interface â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  FastAPI Backend â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Ollama (Llama   â”‚
â”‚   (Browser)     â”‚                       â”‚    (Python)      â”‚                     â”‚     3.2)        â”‚
â”‚                 â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                 â”‚
â”‚  HTML/CSS/JS    â”‚   Real-time Response  â”‚ FastAPI + Python â”‚   Test Cases Stream â”‚  Llama 3.2 Modelâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3-Layer Architecture (A.N.T.)

1. **Architecture Layer** (`architecture/`)
   - Technical SOPs defining goals, inputs, and logic
   - `generation_sop.md` - Test generation specifications

2. **Navigation Layer** (`app/main.py`)
   - FastAPI backend routing requests
   - Applies prompt templates to user input
   - Streams responses from Ollama

3. **Tools Layer** (`tools/`)
   - `check_ollama.py` - Health check and model verification
   - Deterministic, atomic scripts

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- [Ollama](https://ollama.ai/) installed and running
- Llama 3.2 model pulled

### Installation

1. **Clone the repository:**
```bash
git clone https://github.com/aman2743/tcgenerationllm.git
cd tcgenerationllm
```

2. **Install Python dependencies:**
```bash
pip install -r requirements.txt
```

3. **Install and start Ollama:**
```bash
# Install Ollama from https://ollama.ai/

# Pull the Llama 3.2 model
ollama pull llama3.2

# Start Ollama server
ollama serve
```

4. **Verify Ollama connection:**
```bash
python tools/check_ollama.py
```

Expected output:
```
âœ… Ollama is RUNNING and reachable.
âœ… Model 'llama3.2' found.
```

### Running the Application

**Option 1: Using the launcher script (Recommended)**
```bash
python run_app.py
```

**Option 2: Manual start**
```bash
# Terminal 1 - Backend
cd app
python main.py

# Terminal 2 - Frontend
cd frontend
python -m http.server 3000
```

Then open your browser to: **http://localhost:3000**

## ğŸ“ Project Structure

```
tcgenerationllm/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ main.py                 # FastAPI backend server
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html              # Main UI
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css           # Glassmorphism styling
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ app.js              # Frontend logic with streaming
â”œâ”€â”€ architecture/
â”‚   â””â”€â”€ generation_sop.md       # Technical SOP for test generation
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ check_ollama.py         # Ollama health check utility
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture_diagram.png # System architecture diagram
â”œâ”€â”€ .gitignore                  # Git ignore rules
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ run_app.py                  # Application launcher
â”œâ”€â”€ BLAST.md                    # B.L.A.S.T. protocol documentation
â”œâ”€â”€ gemini.md                   # Project constitution
â”œâ”€â”€ task_plan.md                # Development phases
â”œâ”€â”€ progress.md                 # Development progress log
â””â”€â”€ README.md                   # This file
```

## ğŸ¯ How It Works

1. **User Input**: Paste your code or requirements into the chat interface
2. **Template Application**: Backend wraps input with a structured prompt template
3. **LLM Processing**: Ollama (Llama 3.2) analyzes and generates test cases
4. **Streaming Response**: Test cases stream back to the UI in real-time
5. **Formatted Output**: Results displayed in a clean, readable format

### Test Case Format

Each generated test case follows this structure:

```
--------------------------------------------------
Test Case ID: TC001
Description: Verify login with valid credentials

Steps:
1. Navigate to login page
2. Enter valid username and password
3. Click login button

Expected Result: User successfully logged in and redirected to dashboard
--------------------------------------------------
```

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|-----------|
| Frontend | Vanilla HTML5, CSS3 (Glassmorphism), JavaScript (ES6+) |
| Backend | Python, FastAPI, Uvicorn |
| AI Model | Ollama (Llama 3.2) |
| Architecture | B.L.A.S.T. Protocol, A.N.T. 3-Layer |

## ğŸ¨ UI Features

- **Glassmorphism Design**: Modern frosted glass effect with blur
- **Dark Mode**: Easy on the eyes with gradient backgrounds
- **Typing Indicators**: Visual feedback during generation
- **Auto-resize Input**: Textarea expands as you type
- **Smooth Animations**: Polished micro-interactions
- **Responsive Layout**: Works on desktop and tablet

## ğŸ“Š Development Methodology

Built using the **B.L.A.S.T.** protocol:

- âœ… **Blueprint**: Requirements and data schema defined
- âœ… **Link**: Ollama connectivity verified
- âœ… **Architect**: 3-layer architecture implemented
- âœ… **Stylize**: Premium UI/UX design applied
- âœ… **Trigger**: Ready for deployment

## ğŸ”§ Configuration

### Changing the Model

Edit `app/main.py` to use a different Ollama model:

```python
class GenerateRequest(BaseModel):
    userInput: str
    model: str = "llama3.2"  # Change this to your preferred model
```

### Customizing the Prompt Template

Edit the `SYSTEM_PROMPT_TEMPLATE` in `app/main.py` to modify how test cases are generated.

## ğŸ› Troubleshooting

**Issue**: "Could not connect to Ollama"
- **Solution**: Ensure Ollama is running (`ollama serve`)

**Issue**: "Model 'llama3.2' NOT found"
- **Solution**: Pull the model (`ollama pull llama3.2`)

**Issue**: Frontend shows JSON parsing error
- **Solution**: Refresh the browser (Ctrl+Shift+R) to clear cache

## ğŸ“ License

MIT License - Feel free to use this project for personal or commercial purposes.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ‘¨â€ğŸ’» Author

**Aman Kumar**
- GitHub: [@aman2743](https://github.com/aman2743)

## ğŸ™ Acknowledgments

- Built with [Ollama](https://ollama.ai/)
- Powered by [Llama 3.2](https://ai.meta.com/llama/)
- Developed using the B.L.A.S.T. protocol

---

**â­ If you find this project useful, please consider giving it a star!**
